============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4           Conv2d           4       [64, 64, 3, 3]      [1, 64, 32, 32]
    5      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    6             ReLU           4       ['No weights']      [1, 64, 32, 32]
    7        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    8           Conv2d           4      [128, 64, 3, 3]     [1, 128, 16, 16]
    9      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   10             ReLU           4       ['No weights']     [1, 128, 16, 16]
   11           Conv2d           4     [128, 128, 3, 3]     [1, 128, 16, 16]
   12      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   13             ReLU           4       ['No weights']     [1, 128, 16, 16]
   14        MaxPool2d           4       ['No weights']       [1, 128, 8, 8]
   15           Conv2d           4     [256, 128, 3, 3]       [1, 256, 8, 8]
   16      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   17             ReLU           4       ['No weights']       [1, 256, 8, 8]
   18           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   19      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   20             ReLU           4       ['No weights']       [1, 256, 8, 8]
   21           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   22      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   23             ReLU           4       ['No weights']       [1, 256, 8, 8]
   24           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   25      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   26             ReLU           4       ['No weights']       [1, 256, 8, 8]
   27        MaxPool2d           4       ['No weights']       [1, 256, 4, 4]
   28           Conv2d           4     [512, 256, 3, 3]       [1, 512, 4, 4]
   29      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   30             ReLU           4       ['No weights']       [1, 512, 4, 4]
   31           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   32      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   33             ReLU           4       ['No weights']       [1, 512, 4, 4]
   34           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   35      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   36             ReLU           4       ['No weights']       [1, 512, 4, 4]
   37           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   38      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   39             ReLU           4       ['No weights']       [1, 512, 4, 4]
   40        MaxPool2d           4       ['No weights']       [1, 512, 2, 2]
   41           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   42      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   43             ReLU           4       ['No weights']       [1, 512, 2, 2]
   44           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   45      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   46             ReLU           4       ['No weights']       [1, 512, 2, 2]
   47           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   48      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   49             ReLU           4       ['No weights']       [1, 512, 2, 2]
   50           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   51      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   52             ReLU           4       ['No weights']       [1, 512, 2, 2]
   53        MaxPool2d           4       ['No weights']       [1, 512, 1, 1]
   54           Linear           2           [512, 512]             [1, 512]
   55             ReLU           2       ['No weights']             [1, 512]
   56          Dropout           2       ['No weights']             [1, 512]
   57           Linear           2           [512, 512]             [1, 512]
   58             ReLU           2       ['No weights']             [1, 512]
   59          Dropout           2       ['No weights']             [1, 512]
   60           Linear           2           [100, 512]             [1, 100]
==================================================================================


===== Result =====
Seed: 1234
Specific bit flip position: None
Layer #0: 1009 / 7375 = 13.6814%, Identity
Layer #1: 910 / 7375 = 12.3390%, Conv2d
Layer #2: 920 / 7375 = 12.4746%, BatchNorm2d
Layer #3: 919 / 7375 = 12.4610%, ReLU
Layer #4: 841 / 7375 = 11.4034%, Conv2d
Layer #5: 891 / 7375 = 12.0814%, BatchNorm2d
Layer #6: 875 / 7375 = 11.8644%, ReLU
Layer #7: 931 / 7375 = 12.6237%, MaxPool2d
Layer #8: 876 / 7375 = 11.8780%, Conv2d
Layer #9: 905 / 7375 = 12.2712%, BatchNorm2d
Layer #10: 933 / 7375 = 12.6508%, ReLU
Layer #11: 841 / 7375 = 11.4034%, Conv2d
Layer #12: 840 / 7375 = 11.3898%, BatchNorm2d
Layer #13: 844 / 7375 = 11.4441%, ReLU
Layer #14: 883 / 7375 = 11.9729%, MaxPool2d
Layer #15: 870 / 7375 = 11.7966%, Conv2d
Layer #16: 861 / 7375 = 11.6746%, BatchNorm2d
Layer #17: 861 / 7375 = 11.6746%, ReLU
Layer #18: 837 / 7375 = 11.3492%, Conv2d
Layer #19: 841 / 7375 = 11.4034%, BatchNorm2d
Layer #20: 834 / 7375 = 11.3085%, ReLU
Layer #21: 832 / 7375 = 11.2814%, Conv2d
Layer #22: 839 / 7375 = 11.3763%, BatchNorm2d
Layer #23: 848 / 7375 = 11.4983%, ReLU
Layer #24: 836 / 7375 = 11.3356%, Conv2d
Layer #25: 828 / 7375 = 11.2271%, BatchNorm2d
Layer #26: 828 / 7375 = 11.2271%, ReLU
Layer #27: 868 / 7375 = 11.7695%, MaxPool2d
Layer #28: 854 / 7375 = 11.5797%, Conv2d
Layer #29: 835 / 7375 = 11.3220%, BatchNorm2d
Layer #30: 830 / 7375 = 11.2542%, ReLU
Layer #31: 831 / 7375 = 11.2678%, Conv2d
Layer #32: 826 / 7375 = 11.2000%, BatchNorm2d
Layer #33: 813 / 7375 = 11.0237%, ReLU
Layer #34: 836 / 7375 = 11.3356%, Conv2d
Layer #35: 822 / 7375 = 11.1458%, BatchNorm2d
Layer #36: 829 / 7375 = 11.2407%, ReLU
Layer #37: 827 / 7375 = 11.2136%, Conv2d
Layer #38: 850 / 7375 = 11.5254%, BatchNorm2d
Layer #39: 853 / 7375 = 11.5661%, ReLU
Layer #40: 882 / 7375 = 11.9593%, MaxPool2d
Layer #41: 845 / 7375 = 11.4576%, Conv2d
Layer #42: 887 / 7375 = 12.0271%, BatchNorm2d
Layer #43: 913 / 7375 = 12.3797%, ReLU
Layer #44: 842 / 7375 = 11.4169%, Conv2d
Layer #45: 914 / 7375 = 12.3932%, BatchNorm2d
Layer #46: 912 / 7375 = 12.3661%, ReLU
Layer #47: 879 / 7375 = 11.9186%, Conv2d
Layer #48: 872 / 7375 = 11.8237%, BatchNorm2d
Layer #49: 882 / 7375 = 11.9593%, ReLU
Layer #50: 905 / 7375 = 12.2712%, Conv2d
Layer #51: 852 / 7375 = 11.5525%, BatchNorm2d
Layer #52: 834 / 7375 = 11.3085%, ReLU
Layer #53: 928 / 7375 = 12.5831%, MaxPool2d
Layer #54: 918 / 7375 = 12.4475%, Linear
Layer #55: 924 / 7375 = 12.5288%, ReLU
Layer #56: 917 / 7375 = 12.4339%, Dropout
Layer #57: 1022 / 7375 = 13.8576%, Linear
Layer #58: 1006 / 7375 = 13.6407%, ReLU
Layer #59: 1022 / 7375 = 13.8576%, Dropout
Layer #60: 1222 / 7375 = 16.5695%, Linear
